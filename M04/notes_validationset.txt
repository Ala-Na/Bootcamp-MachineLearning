Definitions of Train, Validation, and Test Datasets

To reiterate the findings from researching the experts above, this section provides unambiguous definitions of the three terms.

Training Dataset: The sample of data used to fit the model.
Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.
Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.

We can make this concrete with a pseudocode sketch:
## split data
#data = ...
#train, validation, test = split(data)
# 
## tune model hyperparameters
#parameters = ...
#for params in parameters:
#model = fit(train, params)
#skill = evaluate(model, validation)
# 
## evaluate final model for comparison with other models
#model = fit(train)
#skill = evaluate(model, test)

The validation dataset may also play a role in other forms of model preparation, such as feature selection.
The final model could be fit on the aggregate of the training and validation datasets.

From https://machinelearningmastery.com/difference-test-validation-datasets/


Cross-validation :

- Training set (60% of the original data set): This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.

- Cross-Validation set (20% of the original data set): This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.

- Test set (20% of the original data set): Now we have chosen our preferred prediction algorithm but we don't know yet how it's going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it's going to perform so we can have an idea about our algorithm's performance on unseen data.

From https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set

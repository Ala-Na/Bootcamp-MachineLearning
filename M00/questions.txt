1. Why do we concatenate a column of ones to the left of the x vector when we use
the linear algebra trick?

To simplify calculus and make a simple multiplication between matrix

2. Why does the loss function square the distances between the data points and their
predicted values?

Because it can be negative otherwise
To dimunish the gaussian noise
More info:
https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error

3. What does the loss functionâ€™s output represent?

The distance between prediction and true values, to evaluate if the model
is accurate or not. (Perfect model = 0 for loss function result)

4. Toward which value do we want the loss function to tend? What would that mean?

0 as it represents a perfect model with no difference between prediction and reality

5. Do you understand why are matrix multiplications are not commutative?

For A, B matrix of m * n and o * p shapes:
The product of A x B (shape m * p if n == o) is not necessary the same as
B x A (shape o * n if m == p), which mean they're not
commutative.
This is the consequence of how matrix multiplication is performed.
